{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Nissen et al., 2023: Severe 21st-century OA in Antarctic MPAs\n",
    "#\n",
    "# script to save surface-referenced potential density as netcdf file\n",
    "# requires full model output\n",
    "# note that some paths are hard-coded throughout the script"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!jupyter nbconvert --to script plot_PAPER2_rho0_fields_save_as_netcdf.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "sys.path.append(\"../pyfesom/\") # add pyfesom to search path\n",
    "sys.path.append(\"../python_gsw_py3/\") \n",
    "import pyfesom as pf\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.colors import LinearSegmentedColormap\n",
    "import numpy as np\n",
    "from matplotlib import cm\n",
    "from netCDF4 import Dataset, MFDataset\n",
    "import pandas as pd\n",
    "from gsw import rho # rho from SA, CT, p\n",
    "from gsw import pt0_from_t # potTemp from SA, t, p (at reference pressure 0)\n",
    "from gsw import pt_from_t # potTemp from SA, t, p and reference pressure\n",
    "from gsw import pot_rho_t_exact # potRho from SA, t, p and reference pressure\n",
    "from gsw import p_from_z # get pressure from z and lat\n",
    "from gsw import CT_from_pt # conservative temp from potTemp and SA\n",
    "from gsw import rho_first_derivatives # first derivatives of rho with respect to SA, CT, p\n",
    "from gsw2 import sigma0_pt0_exact # gsw_sigma0_pt0_exact(SA,pt0)\n",
    "#from gsw import alpha_wrt_CT_t_exact #alpha_wrt_CT_t_exact(SA, t, p)\n",
    "#from gsw import alpha_wrt_pt_t_exact #alpha_wrt_CT_t_exact(SA, t, p)\n",
    "#from gsw import beta_const_pt_t_exact #beta_const_pt_t_exact(SA, t, p)\n",
    "from gsw import SA_from_SP\n",
    "from numba import njit\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#\n",
    "# from /global/homes/c/cnissen/scripts/python_gsw_py3/gsw/gibbs//thermodynamics_from_t.py\n",
    "\n",
    "load_gsw_functions = False\n",
    "if load_gsw_functions:\n",
    "    sfac = 0.0248826675584615\n",
    "    \"\"\"\n",
    "    sfac = 1 / (40 * uPS) = 1 / (40. * (SSO / 35.))\n",
    "    \"\"\"\n",
    "\n",
    "    SSO = 35.16504\n",
    "    \"\"\"\n",
    "    SSO is the Standard Ocean Reference Salinity (35.16504 g/kg.)\n",
    "\n",
    "    SSO is the best estimate of the Absolute Salinity of Standard Seawater\n",
    "    when the seawater sample has a Practical Salinity, SP, of 35\n",
    "    (Millero et al., 2008), and this number is a fundamental part of the\n",
    "    TEOS-10 definition of seawater.\n",
    "    \"\"\"\n",
    "\n",
    "    def sigma0_pt0_exact(SA, pt0):\n",
    "        \"\"\"\n",
    "        Calculates potential density anomaly with reference sea pressure of\n",
    "        zero (0) dbar.  The temperature input to this function is potential\n",
    "        temperature referenced to zero dbar.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        SA : array_like\n",
    "             Absolute salinity [g kg :sup:`-1`]\n",
    "        pt0 : array_like\n",
    "              potential temperature [:math:`^\\circ` C (ITS-90)]\n",
    "              with respect to a reference sea pressure of 0 dbar\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        sigma0_pt0_exact : array_like\n",
    "                           potential density anomaly [kg m :sup:`-3`]\n",
    "                           respect to a reference pressure of 0 dbar\n",
    "\n",
    "        Examples\n",
    "        --------\n",
    "        >>> import gsw\n",
    "        >>> SA = [34.7118, 34.8915, 35.0256, 34.8472, 34.7366, 34.7324]\n",
    "        >>> pt0 = [28.7832, 28.4209, 22.7850, 10.2305, 6.8292, 4.3245]\n",
    "        >>> gsw.sigma0_pt0_exact(SA, pt0)\n",
    "        array([ 21.79814475,  22.05251193,  23.89356369,  26.66762521,\n",
    "                27.10723499,  27.4096324 ])\n",
    "\n",
    "        References\n",
    "        ----------\n",
    "        .. [1] IOC, SCOR and IAPSO, 2010: The international thermodynamic equation\n",
    "           of seawater - 2010: Calculation and use of thermodynamic properties.\n",
    "           Intergovernmental Oceanographic Commission, Manuals and Guides No. 56,\n",
    "           UNESCO (English), 196 pp. See Eqn. (3.6.1).\n",
    "        \"\"\"\n",
    "\n",
    "        SA = np.maximum(SA, 0)  # Ensure that SA is non-negative.\n",
    "        x2 = sfac * SA\n",
    "        x = np.sqrt(x2)\n",
    "        y = pt0 * 0.025\n",
    "        g03 = (100015.695367145 +\n",
    "               y * (-270.983805184062 +\n",
    "               y * (1455.0364540468 +\n",
    "               y * (-672.50778314507 +\n",
    "               y * (397.968445406972 +\n",
    "               y * (-194.618310617595 +\n",
    "               y * (63.5113936641785 -\n",
    "               y * 9.63108119393062)))))))\n",
    "        g08 = x2 * (-3310.49154044839 +\n",
    "                    x * (199.459603073901 +\n",
    "                    x * (-54.7919133532887 +\n",
    "                    x * 36.0284195611086 -\n",
    "                    y * 22.6683558512829) +\n",
    "                    y * (-175.292041186547 +\n",
    "                    y * (383.058066002476 +\n",
    "                    y * (-460.319931801257 +\n",
    "                    y * 234.565187611355)))) +\n",
    "                    y * (729.116529735046 +\n",
    "                    y * (-860.764303783977 +\n",
    "                    y * (694.244814133268 +\n",
    "                    y * (-297.728741987187)))))\n",
    "\n",
    "        # The above code is exactly the same as the following two lines of code.\n",
    "        # sigma0_pt_exact = rho_t_exact(SA, pt0, 0.) - 1000\n",
    "\n",
    "        return 100000000. / (g03 + g08) - 1000.0\n",
    "\n",
    "    def check_input(SP, p, lon, lat):\n",
    "        \"\"\"\n",
    "        Check for out of range values.\n",
    "        \"\"\"\n",
    "\n",
    "        # Helper for the \"from_SP\" functions.\n",
    "        lon, lat, p, SP = np.broadcast_arrays(lon, lat, p, SP, subok=True)\n",
    "\n",
    "        cond1 = ((p < 100) & (SP > 120))\n",
    "        cond2 = ((p >= 100) & (SP > 42))\n",
    "        if cond1.any() or cond2.any():  # don't modify input array\n",
    "            mask = np.ma.filled(cond1, False) | np.ma.filled(cond2, False)\n",
    "            SP = np.ma.array(SP, mask=mask)\n",
    "\n",
    "        lon = lon % 360\n",
    "\n",
    "        # FIXME: If we do keep the checks below, they need to\n",
    "        # be reformulated with ValueError('pressure out of range') etc.\n",
    "        # The original also checks for 9999s--a fossil from old-time\n",
    "        # Fortran days.\n",
    "\n",
    "        # I don't think we need these here; if any such checking is\n",
    "        # needed, it should not just be for the \"from_SP\" functions.\n",
    "        if False:\n",
    "            if ((p < -1.5) | (p > 12000)).any():\n",
    "                raise Exception('Sstar_from_SP: pressure is out of range')\n",
    "            if ((lon < 0) | (lon > 360)).any():\n",
    "                raise Exception('Sstar_from_SP: longitude is out of range')\n",
    "            if (np.abs(lat) > 90).any():\n",
    "                raise Exception('Sstar_from_SP: latitude is out of range')\n",
    "\n",
    "        SP = np.maximum(SP, 0)  # Works on masked array also.\n",
    "\n",
    "        return SP, p, lon, lat\n",
    "\n",
    "    def SA_from_SP(SP, p, lon, lat):\n",
    "        \"\"\"Calculates Absolute Salinity from Practical Salinity.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        SP : array_like\n",
    "             salinity (PSS-78) [unitless]\n",
    "        p : array_like\n",
    "            pressure [dbar]\n",
    "        lon : array_like\n",
    "              decimal degrees east [0..+360] or [-180..+180]\n",
    "        lat : array_like\n",
    "              decimal degrees (+ve N, -ve S) [-90..+90]\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        SA : masked array\n",
    "             Absolute salinity [g kg :sup:`-1`]\n",
    "\n",
    "        Notes\n",
    "        -----\n",
    "        The mask is only set when the observation is well and truly on dry\n",
    "        land; often the warning flag is not set until one is several hundred\n",
    "        kilometers inland from the coast.\n",
    "\n",
    "        Since SP is non-negative by definition, this function changes any negative\n",
    "        input values of SP to be zero.\n",
    "\n",
    "        Examples\n",
    "        --------\n",
    "        >>> import gsw\n",
    "        >>> SP = [34.5487, 34.7275, 34.8605, 34.6810, 34.5680, 34.5600]\n",
    "        >>> p = [10, 50, 125, 250, 600, 1000]\n",
    "        >>> lon = 188\n",
    "        >>> lat = 4\n",
    "        >>> gsw.SA_from_SP(SP, p, lon, lat)\n",
    "        array([ 34.71177834,  34.89152262,  35.02554486,  34.84722903,\n",
    "                34.73662847,  34.73236307])\n",
    "\n",
    "        References\n",
    "        ----------\n",
    "        .. [1] IOC, SCOR and IAPSO, 2010: The international thermodynamic equation\n",
    "           of seawater - 2010: Calculation and use of thermodynamic properties.\n",
    "           Intergovernmental Oceanographic Commission, Manuals and Guides No. 56,\n",
    "           UNESCO (English), 196 pp. See section 2.5 and appendices A.4 and A.5.\n",
    "\n",
    "        .. [2] McDougall, T.J., D.R. Jackett and F.J. Millero, 2010: An algorithm\n",
    "           for estimating Absolute Salinity in the global ocean. Submitted to Ocean\n",
    "           Science. A preliminary version is available at Ocean Sci. Discuss.,\n",
    "           6, 215-242.\n",
    "           http://www.ocean-sci-discuss.net/6/215/2009/osd-6-215-2009-print.pdf\n",
    "        \"\"\"\n",
    "\n",
    "        SP, p, lon, lat = check_input(SP, p, lon, lat)\n",
    "\n",
    "        SA = (SSO / 35) * SP * (1 + SAAR(p, lon, lat)[0])\n",
    "        SA_baltic = SA_from_SP_Baltic(SP, lon, lat)\n",
    "\n",
    "        # The following function (SAAR) finds SAAR in the non-Baltic parts of\n",
    "        # the world ocean.  (Actually, this SAAR look-up table returns values\n",
    "        # of zero in the Baltic Sea since SAAR in the Baltic is a function of SP,\n",
    "        # not space.\n",
    "        if SA_baltic is not None:\n",
    "            SA[~SA_baltic.mask] = SA_baltic[~SA_baltic.mask]\n",
    "\n",
    "        return SA\n",
    "\n",
    "    h006 = -2.1078768810e-9\n",
    "    h007 =  2.8019291329e-10\n",
    "\n",
    "\n",
    "    class SA_table(object):\n",
    "        \"\"\"\n",
    "        TODO: Write docstring.\n",
    "        \"\"\"\n",
    "        # Central America barrier\n",
    "        x_ca = np.array([260.0, 272.59, 276.5, 278.65, 280.73, 295.217])\n",
    "        y_ca = np.array([19.55, 13.97, 9.6, 8.1, 9.33, 0.0])\n",
    "\n",
    "        def __init__(self, fname=\"/global/homes/c/cnissen/scripts/python_gsw_py3/gsw/utilities/data/gsw_data_v3_0.npz\", max_p_fudge=10000,\n",
    "                     min_frac=0):\n",
    "            self.fname = fname\n",
    "            self.max_p_fudge = max_p_fudge\n",
    "            self.min_frac = min_frac\n",
    "            data = read_data(fname)\n",
    "            self.lon = data.longs_ref.astype(np.float)\n",
    "            self.lat = data.lats_ref.astype(np.float)\n",
    "            self.p = data.p_ref                # Depth levels\n",
    "            self.dlon = self.lon[1] - self.lon[0]\n",
    "            self.dlat = self.lat[1] - self.lat[0]\n",
    "            self.i_ca, self.j_ca = self.xy_to_ij(self.x_ca, self.y_ca)\n",
    "            # Make the order x, y, z:\n",
    "            # Start with deltaSA_ref (was delta_SA_ref in V2):\n",
    "            temp = data.deltaSA_ref.transpose((2, 1, 0)).copy()\n",
    "            self.dsa_ref = np.ma.masked_invalid(temp)\n",
    "            self.dsa_ref.data[self.dsa_ref.mask] = 0\n",
    "            # Now SAAR_ref, which did not exist in V2:\n",
    "            temp = data.SAAR_ref.transpose((2, 1, 0)).copy()\n",
    "            self.SAAR_ref = np.ma.masked_invalid(temp)\n",
    "            self.SAAR_ref.data[self.SAAR_ref.mask] = 0\n",
    "\n",
    "        def xy_to_ij(self, x, y):\n",
    "            \"\"\"\n",
    "            Convert from lat/lon to grid index coordinates,\n",
    "            without truncation or rounding.\n",
    "            \"\"\"\n",
    "            i = (x - self.lon[0]) / self.dlon\n",
    "            j = (y - self.lat[0]) / self.dlat\n",
    "            return i, j\n",
    "\n",
    "        def _central_america(self, di, dj, ii, jj, gm):\n",
    "            \"\"\"\n",
    "            Use a line running through Central America to zero\n",
    "            the goodmask for grid points in the Pacific forming\n",
    "            the grid box around input locations in the Atlantic,\n",
    "            and vice-versa.\n",
    "            \"\"\"\n",
    "            ix, jy = ii[0] + di, jj[0] + dj  # Reconstruction: minor inefficiency.\n",
    "            inear = ((ix >= self.i_ca[0]) & (ix <= self.i_ca[-1])\n",
    "                     & (jy >= self.j_ca[-1]) & (jy <= self.j_ca[0]))\n",
    "            if not inear.any():\n",
    "                return gm\n",
    "            inear_ind = inear.nonzero()[0]\n",
    "            ix = ix[inear]\n",
    "            jy = jy[inear]\n",
    "            ii = ii[:, inear]\n",
    "            jj = jj[:, inear]\n",
    "            jy_ca = np.interp(ix, self.i_ca, self.j_ca)\n",
    "            above = jy - jy_ca  # > 0 if input point is above dividing line\n",
    "            # Intersections of left and right grid lines with dividing line\n",
    "            jleft_ca = np.interp(ii[0], self.i_ca, self.j_ca)\n",
    "            jright_ca = np.interp(ii[1], self.i_ca, self.j_ca)\n",
    "            jgrid_ca = [jleft_ca, jright_ca, jright_ca, jleft_ca]\n",
    "            # Zero the goodmask for grid points on opposite side of divider\n",
    "            for i in range(4):\n",
    "                opposite = (above * (jj[i] - jgrid_ca[i])) < 0\n",
    "                gm[i, inear_ind[opposite]] = 0\n",
    "            return gm\n",
    "\n",
    "        def xy_interp(self, di, dj, ii, jj, k):\n",
    "            \"\"\"\n",
    "            2-D interpolation, bilinear if all 4 surrounding\n",
    "            grid points are present, but treating missing points\n",
    "            as having the average value of the remaining grid\n",
    "            points. This matches the matlab V2 behavior.\n",
    "            \"\"\"\n",
    "            # Array of weights, CCW around the grid box\n",
    "            w = np.vstack(((1 - di) * (1 - dj),  # lower left\n",
    "                          di * (1 - dj),         # lower right\n",
    "                          di * dj,               # upper right\n",
    "                          (1 - di) * dj))        # upper left\n",
    "            gm = ~self.dsa.mask[ii, jj, k]   # gm is \"goodmask\"\n",
    "            gm = self._central_america(di, dj, ii, jj, gm)\n",
    "            # Save a measure of real interpolation quality.\n",
    "            frac = (w * gm).sum(axis=0)\n",
    "            # Now loosen the interpolation, allowing a value to\n",
    "            # be calculated on a grid point that is masked.\n",
    "            # This matches the matlab gsw version 2 behavior.\n",
    "            jm_partial = gm.any(axis=0) & (~(gm.all(axis=0)))\n",
    "            # The weights of the unmasked points will be increased\n",
    "            # by the sum of the weights of the masked points divided\n",
    "            # by the number of unmasked points in the grid square.\n",
    "            # This is equivalent to setting the masked data values\n",
    "            # to the average of the unmasked values, and then\n",
    "            # unmasking, which is the matlab v2 implementation.\n",
    "            if jm_partial.any():\n",
    "                w_bad = w * (~gm)\n",
    "                w[:, jm_partial] += (w_bad[:, jm_partial].sum(axis=0) /\n",
    "                                     gm[:, jm_partial].sum(axis=0))\n",
    "            w *= gm\n",
    "            wsum = w.sum(axis=0)\n",
    "            valid = wsum > 0  # Only need to prevent division by zero here.\n",
    "            w[:, valid] /= wsum[valid]\n",
    "            w[:, ~valid] = 0\n",
    "            vv = self.dsa.data[ii, jj, k]\n",
    "            vv *= w\n",
    "            dsa = vv.sum(axis=0)\n",
    "            return dsa, frac\n",
    "\n",
    "        def _delta_SA(self, p, lon, lat):\n",
    "            \"\"\"\n",
    "            Table lookup engine--to be called only from SAAR or SA_ref.\n",
    "            \"\"\"\n",
    "            p = np.ma.masked_less(p, 0)\n",
    "            mask_in = np.ma.mask_or(np.ma.getmask(p), np.ma.getmask(lon))\n",
    "            mask_in = np.ma.mask_or(mask_in, np.ma.getmask(lat))\n",
    "            p, lon, lat = [np.ma.filled(a, 0).astype(float) for a in (p, lon, lat)]\n",
    "            p, lon, lat = np.broadcast_arrays(p, lon, lat)\n",
    "            if p.ndim > 1:\n",
    "                shape_in = p.shape\n",
    "                p, lon, lat = list(map(np.ravel, (p, lon, lat)))\n",
    "                reshaped = True\n",
    "            else:\n",
    "                reshaped = False\n",
    "            p_orig = p.copy()  # Save for comparison to clipped p.\n",
    "            ix0, iy0 = self.xy_to_ij(lon, lat)\n",
    "            i0raw = np.floor(ix0).astype(int)\n",
    "            i0 = np.clip(i0raw, 0, len(self.lon) - 2)\n",
    "            di = ix0 - i0\n",
    "            j0raw = np.floor(iy0).astype(int)\n",
    "            j0 = np.clip(j0raw, 0, len(self.lat) - 2)\n",
    "            dj = iy0 - j0\n",
    "            # Start at lower left and go CCW; match order in _xy_interp.\n",
    "            ii = np.vstack((i0, i0 + 1, i0 + 1, i0))\n",
    "            jj = np.vstack((j0, j0, j0 + 1, j0 + 1))\n",
    "            k1 = np.searchsorted(self.p, p, side='right')\n",
    "            # Clip p and k1 at max p of grid cell.\n",
    "            kmax = (self.ndepth[ii, jj].max(axis=0) - 1)\n",
    "            mask_out = kmax.mask\n",
    "            kmax = kmax.filled(1)\n",
    "            clip_p = (p >= self.p[kmax])\n",
    "            p[clip_p] = self.p[kmax[clip_p]]\n",
    "            k1[clip_p] = kmax[clip_p]\n",
    "            k0 = k1 - 1\n",
    "            dsa0, frac0 = self.xy_interp(di, dj, ii, jj, k0)\n",
    "            dsa1, frac1 = self.xy_interp(di, dj, ii, jj, k1)\n",
    "            dp = np.diff(self.p)\n",
    "            pfrac = (p - self.p[k0]) / dp[k0]\n",
    "            delta_SA = dsa0 * (1 - pfrac) + dsa1 * pfrac\n",
    "            # Save intermediate results in case we are curious about\n",
    "            # them; the frac values are most likely to be useful.\n",
    "            # We won't bother to reshape them, though, and we may\n",
    "            # delete them later.\n",
    "            self.dsa0 = dsa0\n",
    "            self.frac0 = frac0\n",
    "            self.dsa1 = dsa1\n",
    "            self.frac1 = frac1\n",
    "            self.pfrac = pfrac\n",
    "            self.p_fudge = p_orig - p\n",
    "            # Editing options, in case we don't want to use\n",
    "            # values calculated from the wrong pressure, or from\n",
    "            # an incomplete SA table grid square.\n",
    "            # mask_out |= self.p_fudge > self.max_p_fudge\n",
    "            # mask_out |= self.frac1 < self.min_frac\n",
    "            # delta_SA = np.ma.array(delta_SA, mask=mask_out, copy=False)\n",
    "            # Later on, it is expected to be a masked array.\n",
    "            delta_SA = np.ma.array(delta_SA, copy=False)\n",
    "            if reshaped:\n",
    "                delta_SA.shape = shape_in\n",
    "                self.p_fudge.shape = shape_in\n",
    "            if mask_in is not np.ma.nomask:\n",
    "                delta_SA = np.ma.array(delta_SA, mask=mask_in, copy=False)\n",
    "            return delta_SA\n",
    "\n",
    "        def SAAR(self, p, lon, lat):\n",
    "            \"\"\"\n",
    "            Table lookup of salinity anomaly ratio, given pressure, lon, and lat.\n",
    "            \"\"\"\n",
    "            self.dsa = self.SAAR_ref\n",
    "            # In V2,\n",
    "            # ndepth from the file disagrees with the unmasked count from\n",
    "            # SAAR_ref in a few places; this should be fixed in the\n",
    "            # file, but for now we will simply calculate ndepth directly from\n",
    "            # SAAR_ref.\n",
    "            # TODO: check to see whether this discrepancy is also found in V3.\n",
    "            # TODO: check: do we even need to calculate ndepth? It doesn't\n",
    "            #       appear to be used for anything.\n",
    "            # self.ndepth = np.ma.masked_invalid(data.ndepth_ref.T).astype(np.int8)\n",
    "            ndepth = self.dsa.count(axis=-1)\n",
    "            self.ndepth = np.ma.masked_equal(ndepth, 0)\n",
    "            return self._delta_SA(p, lon, lat)\n",
    "\n",
    "        def delta_SA_ref(self, p, lon, lat):\n",
    "            \"\"\"\n",
    "            Table lookup of salinity anomaly reference value, given pressure,\n",
    "            lon, and lat.\n",
    "            \"\"\"\n",
    "            self.dsa = self.dsa_ref\n",
    "            # See comment in previous method.\n",
    "            ndepth = self.dsa.count(axis=-1)\n",
    "            self.ndepth = np.ma.masked_equal(ndepth, 0)\n",
    "            return self._delta_SA(p, lon, lat)\n",
    "\n",
    "    def SAAR(p, lon, lat):\n",
    "        \"\"\"\n",
    "        Absolute Salinity Anomaly Ratio (excluding the Baltic Sea).\n",
    "        Calculates the Absolute Salinity Anomaly Ratio, SAAR, in the open ocean\n",
    "        by spatially interpolating the global reference data set of SAAR to the\n",
    "        location of the seawater sample.\n",
    "        This function uses version 3.0 of the SAAR look up table.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        p : array_like\n",
    "            pressure [dbar]\n",
    "        lon : array_like\n",
    "              decimal degrees east (will be treated modulo 360)\n",
    "        lat : array_like\n",
    "              decimal degrees (+ve N, -ve S) [-90..+90]\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        SAAR : array\n",
    "               Absolute Salinity Anomaly Ratio [unitless]\n",
    "        in_ocean : boolean array\n",
    "\n",
    "        Notes\n",
    "        -----\n",
    "        The Absolute Salinity Anomaly Ratio in the Baltic Sea is evaluated\n",
    "        separately, since it is a function of Practical Salinity, not of space.\n",
    "        The present function returns a SAAR of zero for data in the Baltic Sea.\n",
    "        The correct way of calculating Absolute Salinity in the Baltic Sea is by\n",
    "        calling SA_from_SP.\n",
    "        The in_ocean flag is only set when the observation is well and truly on dry\n",
    "        land; often the warning flag is not set until one is several hundred\n",
    "        kilometers inland from the coast.\n",
    "\n",
    "        The algorithm is taken from the matlab implementation of the references,\n",
    "        but the numpy implementation here differs substantially from the\n",
    "        matlab implementation.\n",
    "\n",
    "        References\n",
    "        ----------\n",
    "        .. [1] IOC, SCOR and IAPSO, 2010: The international thermodynamic equation\n",
    "           of seawater - 2010: Calculation and use of thermodynamic properties.\n",
    "           Intergovernmental Oceanographic Commission, Manuals and Guides No. 56,\n",
    "           UNESCO (English), 196 pp.\n",
    "\n",
    "        .. [2] McDougall, T.J., D.R. Jackett and F.J. Millero, 2010: An algorithm\n",
    "           for estimating Absolute Salinity in the global ocean.  Submitted to\n",
    "           Ocean Science. A preliminary version is available at Ocean Sci.\n",
    "           Discuss., 6, 215-242.\n",
    "           http://www.ocean-sci-discuss.net/6/215/2009/osd-6-215-2009-print.pdf\n",
    "        \"\"\"\n",
    "\n",
    "        saar = SA_table().SAAR(p, lon, lat)\n",
    "        return saar, ~saar.mask\n",
    "\n",
    "    def read_data(fname, datadir=None):\n",
    "        \"\"\"\n",
    "        Read variables from a numpy '.npz' file into a minimal class providing\n",
    "        attribute access.  A cache is used to avoid re-reading the same file.\n",
    "        \"\"\"\n",
    "        return _npz_cache(fname, datadir=datadir)\n",
    "\n",
    "    class Cache_npz(object):\n",
    "        def __init__(self):\n",
    "            self._cache = dict()\n",
    "            self._default_path = os.path.join(os.path.dirname(__file__), 'data')\n",
    "\n",
    "        def __call__(self, fname, datadir=None):\n",
    "            if datadir is None:\n",
    "                datadir = self._default_path\n",
    "            fpath = os.path.join(datadir, fname)\n",
    "            try:\n",
    "                return self._cache[fpath]\n",
    "            except KeyError:\n",
    "                pass\n",
    "            d = np.load(fpath)\n",
    "            ret = Bunch(d)\n",
    "            self._cache[fpath] = ret\n",
    "            return ret\n",
    "\n",
    "    _npz_cache = Cache_npz()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#---\n",
    "# some settings\n",
    "#-----\n",
    "\n",
    "meshpath = '/pscratch/sd/c/cnissen/mesh_COARZE/'\n",
    "mesh = pf.load_mesh(meshpath, get3d=True,usepickle=False)\n",
    "#print(mesh.n32)\n",
    "\n",
    "year_list    = np.arange(1990,2100+1,1) #np.arange(2010,2089+1,1)\n",
    "print (year_list)\n",
    "    \n",
    "ref_pressure = 0 # currently, the script has been set up for 0 & 1000 & 2000\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#---\n",
    "# load mesh info\n",
    "#---\n",
    "\n",
    "path_mesh = '/pscratch/sd/c/cnissen/'\n",
    "file_mesh = 'Nissen2022_FESOM_REcoM_mesh_information_corrected_20220910.nc'\n",
    "\n",
    "f1 = Dataset(path_mesh+file_mesh) #xr.open_dataset(path+file1)\n",
    "lats      = f1.variables['lat'][:]\n",
    "lons      = f1.variables['lon'][:]\n",
    "zlevs    = f1.variables['zlevs'][:]\n",
    "cavities = f1.variables['cavity'][:]\n",
    "topo     = f1.variables['topo'][:]\n",
    "area_nodes     = f1.variables['cell_area'][:]\n",
    "volume   = f1.variables['cell_volume'][:]\n",
    "f1.close()\n",
    "print(lats.shape)\n",
    "\n",
    "ind_no_cavity = np.where(cavities==0)[0]\n",
    "ind_cavities = np.where(cavities==1)[0]\n",
    "\n",
    "#\n",
    "df = pd.read_csv('/pscratch/sd/c/cnissen/HLRN_runs_postprocessed/nod3d.out', delim_whitespace=True, skiprows=1, \\\n",
    "                        names=['node_number','x','y','z','flag'])\n",
    "lats3d    = df.y.values\n",
    "lons3d    = df.x.values\n",
    "zlevs3d = df.z.values\n",
    "print (zlevs3d.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#-----\n",
    "# get pressure array and convert to depths x nodes\n",
    "#-----\n",
    "\n",
    "if np.min(zlevs3d)<-10:\n",
    "    p = p_from_z(zlevs3d,lats3d)\n",
    "elif np.min(zlevs3d)>=0:\n",
    "    p = p_from_z(-1*zlevs3d,lats3d) # to make sure that pressure is >0\n",
    "print (np.min(p),np.max(p),p.shape)  \n",
    "\n",
    "# convert mesh.n3d to depth x mesh.n2d\n",
    "# convert mesh.n3d to depth x mesh.n2d\n",
    "depth = np.unique(-1*zlevs)\n",
    "p_2d   = np.nan*np.ones([len(depth),len(lons)])\n",
    "lat_2d = np.nan*np.ones([len(depth),len(lons)]) #-999*np.ones\n",
    "lon_2d = np.nan*np.ones([len(depth),len(lons)])\n",
    "for dd in tqdm(range(0,len(depth))): # loop over depths\n",
    "    for ii in range(0,len(lons)): # loop over surface nodes\n",
    "        if mesh.n32[ii,dd]>0:\n",
    "            #volume_levelwise[dd,ii] = volume_nodes[mesh.n32[ii,dd]-1]\n",
    "            p_2d[dd,ii]   = p[mesh.n32[ii,dd]-1]\n",
    "            lat_2d[dd,ii] = lats3d[mesh.n32[ii,dd]-1]\n",
    "            lon_2d[dd,ii] = lons3d[mesh.n32[ii,dd]-1]\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#----\n",
    "# function\n",
    "#----\n",
    "\n",
    "@njit\n",
    "def reorganize_field_in_cavities(ind_cavities,data): \n",
    "    for ii in ind_cavities:  # [72408]\n",
    "        bb = data[:,ii] # get all depth levels at current cavity node\n",
    "        ind_av = np.where(bb>0)[0] # get indices of all depth levels that are NOT masked\n",
    "        ##print bb\n",
    "        #print ind_av\n",
    "        #ind_av = bb>=0 #bb.mask==False\n",
    "        #nd_av = np.where(ind_av==True)[0]\n",
    "        #print ind_av\n",
    "        # if surface value is filled, but thereafter there is a gap: \n",
    "        if len(ind_av)>1:\n",
    "            if (ind_av[1]-ind_av[0])>1:  #any(np.diff(ind_av)>1):  \n",
    "                bb[ind_av[1]-1]=bb[ind_av[0]] # move \"surface\" value to correct depth\n",
    "                bb[ind_av[0]] = 0 # set surface entry to zero\n",
    "               \n",
    "        data[:,ii] = bb # overwrite original field\n",
    "    return data\n",
    "\n",
    "@njit\n",
    "def reorganize_pressure_field_in_cavities(ind_cavities,data,data_pressure): \n",
    "    #------\n",
    "    # NOTE: \"data\" should be the data that are already reorganized in cavities!!!!\n",
    "    #       \"data\" should not have masked values, but zeros where there is not data\n",
    "    #------\n",
    "    for ii in ind_cavities:  #[72408]:\n",
    "        cc = data_pressure[:,ii]\n",
    "        bb = data[:,ii] # get all depth levels at current cavity node\n",
    "        ind_av = np.where(bb>0)[0] # get indices of all depth levels that are NOT masked\n",
    "        #print ind_av\n",
    "        cc_aux = np.zeros_like(cc)\n",
    "        cc_aux[ind_av] = cc[0:len(ind_av)]\n",
    "        data_pressure[:,ii] = cc_aux # overwrite original field\n",
    "        \n",
    "    return data_pressure\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#------\n",
    "# get monthly density for each year\n",
    "#------\n",
    "\n",
    "correct_pressure_array = True\n",
    "for yy in range(0,len(year_list)):\n",
    "    year = year_list[yy]\n",
    "    print ('Load output for year '+str(year)+'...')\n",
    "    \n",
    "    if year<2015: \n",
    "        path1  = '/pscratch/sd/c/cnissen/COARZE_temp/hist/'\n",
    "        path2  = '/pscratch/sd/c/cnissen/COARZE_salt/hist/'\n",
    "    else: \n",
    "        path1  = '/pscratch/sd/c/cnissen/COARZE_temp/ssp585/'\n",
    "        path2  = '/pscratch/sd/c/cnissen/COARZE_salt/ssp585/'\n",
    "        \n",
    "    temp_file    = Dataset(path1+'thetao_fesom_'+str(year)+'0101.nc') \n",
    "    salt_file    = Dataset(path2+'so_fesom_'+str(year)+'0101.nc') \n",
    "    \n",
    "    for mm in tqdm(range(0,12)):\n",
    "        \n",
    "        # load data \n",
    "        temp    = temp_file.variables['thetao'][mm,:,:] # potential temp, depth x nodes\n",
    "        salt_sp = salt_file.variables['so'][mm,:,:] # salinity, , depth x nodes\n",
    "        \n",
    "        #------\n",
    "        # calculate density, reference pressure 0\n",
    "        #------\n",
    "        # set masked values to 0 to get correction within cavity correct\n",
    "        # (if I don't do that, masked and not-masked values are not correctly recognized with njit)\n",
    "        salt_sp[salt_sp.mask==True]=0 \n",
    "        temp[temp.mask==True]=0 \n",
    "        # move \"surface\" value in cavities to correct depth\n",
    "        salt_sp = reorganize_field_in_cavities(ind_cavities,salt_sp) \n",
    "        temp    = reorganize_field_in_cavities(ind_cavities,temp)\n",
    "        if correct_pressure_array:\n",
    "            # correct pressure array \n",
    "            print ('Correct pressure array in cavities')\n",
    "            p_2d[np.isnan(p_2d)]=0\n",
    "            lon_2d[np.isnan(lon_2d)]=0\n",
    "            lat_2d[np.isnan(lat_2d)]=0\n",
    "            # (temp is used to extract info at what depth levels pressure should be, temp should not be masked!)\n",
    "            p_2d = reorganize_pressure_field_in_cavities(ind_cavities,salt_sp,p_2d)\n",
    "            lon_2d = reorganize_pressure_field_in_cavities(ind_cavities,salt_sp,lon_2d)\n",
    "            lat_2d = reorganize_pressure_field_in_cavities(ind_cavities,salt_sp,lat_2d)\n",
    "            # set zeros back to masked\n",
    "            p_2d = np.ma.masked_where(p_2d==0,p_2d)\n",
    "            p_2d[0,:] = 0 # surface layer should actually have zero pressure!!\n",
    "            #\n",
    "            # BUG fix Dec 2023: in the lines below, the lon_2d/lat_2d fields were overwritten with pressure fields!!!!\n",
    "            # re-calculate rho fields for all the years!\n",
    "            #\n",
    "            lon_2d = np.ma.masked_where(lon_2d==0,lon_2d) #p_2d)\n",
    "            lat_2d = np.ma.masked_where(lat_2d==0,lat_2d) #p_2d)\n",
    "            correct_pressure_array = False # set to False as I only need to do this once!\n",
    "        \n",
    "        # set zeros back to masked\n",
    "        salt_sp = np.ma.masked_where(salt_sp==0,salt_sp) \n",
    "        temp = np.ma.masked_where(temp==0,temp)\n",
    "           \n",
    "        salt_abs = np.zeros_like(salt_sp)\n",
    "        for dd in range(0,salt_sp.shape[0]): # loop over depths\n",
    "            salt_abs[dd,:] = SA_from_SP(salt_sp[dd,:], p_2d[dd,:], lon_2d[dd,:],lat_2d[dd,:]) # the resulting salt_abs field looks weird!\n",
    "        \n",
    "        sigma = sigma0_pt0_exact(salt_abs,temp)  # from abs salinity and pot temp\n",
    "        rho = np.squeeze(sigma) + 1000\n",
    "        #del sigma,salt_abs,temp,salt_sp\n",
    "        \n",
    "        # save as netcdf\n",
    "        save_to_netcdf = True\n",
    "        if save_to_netcdf:\n",
    "            \n",
    "            vari = 'rho0'\n",
    "\n",
    "            savepath = '/pscratch/sd/c/cnissen/HLRN_runs_postprocessed/PAPER2_postprocessed/rho0_fields/'\n",
    "            source = '/global/homes/c/cnissen/scripts/plot_FESOM_rho0_fields_save_as_netcdf.ipynb'\n",
    "\n",
    "            netcdf_name = vari+'_fesom_'+str(year)+'0101.nc'\n",
    "\n",
    "            if not os.path.exists(savepath+netcdf_name):\n",
    "                print ('Create file '+savepath+netcdf_name)\n",
    "                w_nc_fid = Dataset(savepath+netcdf_name, 'w', format='NETCDF4_CLASSIC')\n",
    "                # create dimension & variable\n",
    "                w_nc_fid.createDimension('nodes_2d', mesh.n2d)\n",
    "                w_nc_fid.createDimension('depth', len(np.unique(zlevs)))\n",
    "                w_nc_fid.createDimension('time', 12)\n",
    "                w_nc_fid.script = source\n",
    "                w_nc_fid.close()\n",
    "\n",
    "            w_nc_fid = Dataset(savepath+netcdf_name, 'r+', format='NETCDF4_CLASSIC')      # Create and open new netcdf file to write to\n",
    "            try:\n",
    "                w_nc_var1 = w_nc_fid.createVariable(vari, 'f4',('time','depth','nodes_2d'))\n",
    "                w_nc_var1.units = 'kg m-3' \n",
    "                w_nc_var1.description = 'Surface-referenced potential density'\n",
    "            except:\n",
    "                pass\n",
    "\n",
    "            w_nc_fid.variables[vari][mm,:,:] = rho\n",
    "                        \n",
    "            w_nc_fid.close() \n",
    "\n",
    "            if mm==11:\n",
    "                print ('Successfully saved '+vari+' for all months in year '+str(year))\n",
    "        #del rho\n",
    "\n",
    "print ('done')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#---\n",
    "# test plot\n",
    "#---\n",
    "test_plot = False\n",
    "if test_plot:\n",
    "    print(rho.shape)\n",
    "\n",
    "    dpicnt = 150\n",
    "\n",
    "    print('Min/Max p_2d:',np.min(p_2d),np.max(p_2d))\n",
    "    fig7= plt.figure(num=18, figsize=(7,3), dpi=dpicnt, facecolor='w', edgecolor='k')     \n",
    "    plt.contourf(p_2d,extend='both',cmap=cm.inferno,levels=np.arange(0,6000+100,100))#,levels=np.arange()) \n",
    "    plt.colorbar()\n",
    "    plt.show()\n",
    "\n",
    "    fig7= plt.figure(num=18, figsize=(7,3), dpi=dpicnt, facecolor='w', edgecolor='k')     \n",
    "    plt.contourf(temp,extend='both',cmap=cm.inferno)#,levels=np.arange()) \n",
    "    plt.show()\n",
    "\n",
    "    fig7= plt.figure(num=18, figsize=(7,3), dpi=dpicnt, facecolor='w', edgecolor='k')     \n",
    "    plt.contourf(salt_sp,extend='both',cmap=cm.inferno)#,levels=np.arange()) \n",
    "    plt.colorbar()\n",
    "    plt.show()\n",
    "\n",
    "    fig7= plt.figure(num=18, figsize=(7,3), dpi=dpicnt, facecolor='w', edgecolor='k')     \n",
    "    plt.contourf(salt_abs,extend='both',cmap=cm.inferno)#,levels=np.arange()) \n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv-jupyter",
   "language": "python",
   "name": "myenv-jupyter"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
